{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagnik-sudo/Workload-Forecasting-Redset/blob/deepar/workload_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48lZh6EJ9HlI"
      },
      "source": [
        "# Amazon Redset Workload Forecasting\n",
        "\n",
        "## What is Redset?\n",
        "Redset is a dataset released by Amazon in 2024, comprising three months of user query metadata from a selected sample of Amazon Redshift instances. It includes query metadata for **200 provisioned and 200 serverless instances**, offering insights into user interactions with these database services. While not representative of the entire Redshift fleet, Redset serves as a valuable resource for developing **new benchmarks and exploring machine learning techniques**, such as **workload forecasting**, tailored to these specific workloads.\n",
        "\n",
        "## What we perform in this notebook?\n",
        "\n",
        "In this notebook, we analyze **Amazon Redset**, a dataset containing query metadata from Amazon Redshift instances, to explore **workload forecasting techniques** for **intelligent resource scaling**. Our primary objectives are:\n",
        "\n",
        "### 1. Baseline Model Evaluation\n",
        "- We evaluate **traditional forecasting baselines**, such as:\n",
        "  - **AutoGluon DeepAR**\n",
        "  - **Seasonal Naive Models**\n",
        "- These models establish reference points for **workload prediction**.\n",
        "\n",
        "### 2. Development of RNN-based Forecasting Models\n",
        "- We implement **Recurrent Neural Network (RNN)-based models** to improve **forecasting accuracy**.\n",
        "- These models aim to **capture complex workload patterns** and **improve upon the baselines**.\n",
        "\n",
        "### 3. Comparison Between Baselines and RNN-based Approaches\n",
        "- Using the **Redset dataset**, we compare the performance of our **custom RNN models** with:\n",
        "  - **AutoGluon DeepAR**\n",
        "  - **Statistical forecasting methods** (e.g., ARIMA, ETS)\n",
        "- We use metrics such as **Q-error** and **forecast accuracy** to assess improvements.\n",
        "\n",
        "---\n",
        "\n",
        "### Reference\n",
        "For more details on the forecasting methodologies and benchmark comparisons, we refer to the **attached paper: \"Forecasting Algorithms for Intelligent Resource Scaling: An Experimental Analysis\"**.  \n",
        "This paper provides insights into **workload forecasting challenges**, evaluation metrics, and strategies for improving predictive accuracy in cloud environments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon.timeseries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ObfJJG_Gzp",
        "outputId": "d55ced34-f8e3-49fc-915b-36225377b878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon.timeseries\n",
            "  Downloading autogluon.timeseries-1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (1.4.2)\n",
            "Requirement already satisfied: numpy<2.1.4,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (1.13.1)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (2.2.2)\n",
            "Requirement already satisfied: torch<2.6,>=2.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (2.5.1+cu124)\n",
            "Collecting lightning<2.6,>=2.2 (from autogluon.timeseries)\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from autogluon.timeseries)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers<5,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.timeseries) (4.48.3)\n",
            "Collecting accelerate<1.0,>=0.34.0 (from autogluon.timeseries)\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries)\n",
            "  Downloading gluonts-0.16.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (3.4.2)\n",
            "Collecting statsforecast<1.8,>=1.7.0 (from autogluon.timeseries)\n",
            "  Downloading statsforecast-1.7.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
            "Collecting mlforecast==0.13.4 (from autogluon.timeseries)\n",
            "  Downloading mlforecast-0.13.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting utilsforecast<0.2.5,>=0.2.3 (from autogluon.timeseries)\n",
            "  Downloading utilsforecast-0.2.4-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting coreforecast==0.0.12 (from autogluon.timeseries)\n",
            "  Downloading coreforecast-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting fugue>=0.9.0 (from autogluon.timeseries)\n",
            "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (4.67.1)\n",
            "Requirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (3.10.15)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries) (2.18.0)\n",
            "Collecting autogluon.core==1.2 (from autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading autogluon.core-1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.common==1.2 (from autogluon.timeseries)\n",
            "  Downloading autogluon.common-1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.2 (from autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries)\n",
            "  Downloading autogluon.tabular-1.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.common==1.2->autogluon.timeseries)\n",
            "  Downloading boto3-1.36.24-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: psutil<7.0.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.2->autogluon.timeseries) (5.9.5)\n",
            "Collecting scikit-learn<1.5.3,>=1.4.0 (from autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.32.3)\n",
            "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (3.10.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[raytune]==1.2->autogluon.timeseries) (17.0.0)\n",
            "Collecting ray<2.40,>=2.10.0 (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading ray-2.39.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[raytune]==1.2->autogluon.timeseries) (0.2.7)\n",
            "Collecting autogluon.features==1.2 (from autogluon.tabular==1.2->autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries)\n",
            "  Downloading autogluon.features-1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting catboost<1.3,>=1.2 (from autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries)\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: lightgbm<4.6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries) (4.5.0)\n",
            "Requirement already satisfied: xgboost<2.2,>=1.6 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries) (2.1.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from mlforecast==0.13.4->autogluon.timeseries) (3.1.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from mlforecast==0.13.4->autogluon.timeseries) (2024.10.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from mlforecast==0.13.4->autogluon.timeseries) (0.61.0)\n",
            "Collecting optuna (from mlforecast==0.13.4->autogluon.timeseries)\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mlforecast==0.13.4->autogluon.timeseries) (24.2)\n",
            "Collecting window-ops (from mlforecast==0.13.4->autogluon.timeseries)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate<1.0,>=0.34.0->autogluon.timeseries) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<1.0,>=0.34.0->autogluon.timeseries) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<1.0,>=0.34.0->autogluon.timeseries) (0.5.2)\n",
            "Collecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries)\n",
            "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries) (2.10.6)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries) (4.12.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.timeseries) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.timeseries) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.timeseries) (2025.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from statsforecast<1.8,>=1.7.0->autogluon.timeseries) (0.14.4)\n",
            "Requirement already satisfied: threadpoolctl>=3 in /usr/local/lib/python3.11/dist-packages (from statsforecast<1.8,>=1.7.0->autogluon.timeseries) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.timeseries) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (3.17.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.6,>=2.2->autogluon.timeseries)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.6,>=2.2->autogluon.timeseries) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.6,>=2.2->autogluon.timeseries) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.38.0->transformers[sentencepiece]<5,>=4.38.0->autogluon.timeseries) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.38.0->transformers[sentencepiece]<5,>=4.38.0->autogluon.timeseries) (0.21.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.timeseries) (0.2.0)\n",
            "Collecting botocore<1.37.0,>=1.36.24 (from boto3<2,>=1.10->autogluon.common==1.2->autogluon.timeseries)\n",
            "  Downloading botocore-1.36.24-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.common==1.2->autogluon.timeseries)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3<2,>=1.10->autogluon.common==1.2->autogluon.timeseries)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries) (5.24.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (3.11.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.0.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (3.2.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->mlforecast==0.13.4->autogluon.timeseries) (0.44.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (8.1.8)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.5.0)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.21.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting memray (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading memray-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast<1.8,>=1.7.0->autogluon.timeseries) (1.0.1)\n",
            "Collecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.9->autogluon.timeseries) (3.0.2)\n",
            "Collecting alembic>=1.5.0 (from optuna->mlforecast==0.13.4->autogluon.timeseries)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->mlforecast==0.13.4->autogluon.timeseries)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries) (2.0.38)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.2->autogluon.core[raytune]==1.2->autogluon.timeseries) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (2.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.6,>=2.2->autogluon.timeseries) (1.18.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->mlforecast==0.13.4->autogluon.timeseries)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast==0.13.4->autogluon.timeseries) (3.1.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (4.3.6)\n",
            "Collecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<2.40,>=2.10.0->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.22.3)\n",
            "Requirement already satisfied: rich>=11.2.0 in /usr/local/lib/python3.11/dist-packages (from memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (13.9.4)\n",
            "Collecting textual>=0.41.0 (from memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading textual-2.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.19.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[catboost,lightgbm,xgboost]==1.2->autogluon.timeseries) (9.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.67.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.27.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.2.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.2.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.18.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.4.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (1.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.40,>=2.10.0; extra == \"raytune\"->autogluon.core[raytune]==1.2->autogluon.timeseries) (0.6.1)\n",
            "Downloading autogluon.timeseries-1.2-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.common-1.2-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.core-1.2-py3-none-any.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.tabular-1.2-py3-none-any.whl (352 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlforecast-0.13.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.features-1.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gluonts-0.16.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-1.7.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:01:15\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from typing import Dict\n",
        "from autogluon.timeseries import TimeSeriesPredictor\n",
        "from typing import List, Dict\n",
        "\n",
        "class DeepAR:\n",
        "    \"\"\"\n",
        "    DeepAR implementation for workload forecasting using AutoGluon TimeSeries.\n",
        "    This implementation explicitly configures AutoGluon to use the DeepAR model.\n",
        "    Supports training, prediction, evaluation, and saving/loading models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prediction_length: int, freq: str = \"h\", hyperparameters: Dict = None):\n",
        "        print(\"Initializing DeepARAutogluonTS Model...\")\n",
        "        self.prediction_length = prediction_length\n",
        "        self.freq = freq\n",
        "        self.model = None\n",
        "\n",
        "        # Default hyperparameters for the DeepAR model in AutoGluon.\n",
        "        # By wrapping the parameters in a dictionary with the key \"DeepAR\",\n",
        "        # we ensure that AutoGluon uses only the DeepAR model.\n",
        "        default_deepar_hp = {\n",
        "            \"epochs\": 50,\n",
        "            \"learning_rate\": 1e-3,\n",
        "            \"num_layers\": 2,\n",
        "            \"hidden_size\": 40,\n",
        "            \"dropout_rate\": 0.1,\n",
        "            \"batch_size\": 32,\n",
        "            \"context_length\": prediction_length,\n",
        "        }\n",
        "        # Explicitly force usage of DeepAR by setting the key.\n",
        "        self.hyperparameters = hyperparameters or {\"DeepAR\": default_deepar_hp}\n",
        "\n",
        "    def prepare_data(self, data: pd.DataFrame, target_column: str = \"query_count\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Converts a pandas DataFrame into the long format expected by AutoGluon TimeSeries.\n",
        "        Assumes input DataFrame has columns 'timestamp' and target_column.\n",
        "        \"\"\"\n",
        "        data = data.copy()\n",
        "        data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
        "        data = data.sort_values(\"timestamp\")\n",
        "        # Add a constant item_id since we are forecasting a single time series.\n",
        "        data[\"item_id\"] = \"item_1\"\n",
        "        # Rearranging columns if needed.\n",
        "        data = data[[\"item_id\", \"timestamp\", target_column]]\n",
        "        return data\n",
        "\n",
        "    def train(self, train_data: pd.DataFrame, target_column: str = \"query_count\"):\n",
        "        \"\"\"\n",
        "        Trains the DeepAR model using AutoGluon TimeSeries.\n",
        "        \"\"\"\n",
        "        prepared_data = self.prepare_data(train_data, target_column)\n",
        "\n",
        "        # Initialize the TimeSeriesPredictor with the target, prediction length, and frequency.\n",
        "        print(\"Training started using DeepAR...\")\n",
        "        self.model = TimeSeriesPredictor(\n",
        "            target=target_column,\n",
        "            prediction_length=self.prediction_length,\n",
        "            freq=self.freq,\n",
        "            eval_metric='WQL'\n",
        "        )\n",
        "\n",
        "        # Fit the predictor with the specified DeepAR hyperparameters.\n",
        "        self.model.fit(\n",
        "            train_data=prepared_data,\n",
        "            hyperparameters=self.hyperparameters,\n",
        "            # You can pass additional parameters such as time_limit if needed.\n",
        "        )\n",
        "        print(\"Training completed using DeepAR.\")\n",
        "\n",
        "    def predict(self, test_data: pd.DataFrame, target_column: str = \"query_count\") -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generates predictions using the trained model.\n",
        "        Returns a DataFrame with timestamps and prediction statistics.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained before making predictions.\")\n",
        "\n",
        "        prepared_data = self.prepare_data(test_data, target_column)\n",
        "        predictions = self.model.predict(prepared_data)\n",
        "\n",
        "        # Extract forecasts for our single time series (item_id = \"item_1\").\n",
        "        # The predictions DataFrame index is 'timestamp' and the columns include the forecast quantiles.\n",
        "        # Here we compute the mean forecast and select quantiles for lower and upper bounds.\n",
        "        forecast = predictions.loc[\"item_1\"]\n",
        "        forecast = forecast.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
        "\n",
        "        # Check if AutoGluon returns quantile columns (they usually have names like '0.1', '0.5', '0.9')\n",
        "        # Here we assume the median is our best estimate for the mean forecast.\n",
        "        lower_bound = forecast[\"0.1\"] if \"0.1\" in forecast.columns else None\n",
        "        upper_bound = forecast[\"0.9\"] if \"0.9\" in forecast.columns else None\n",
        "        mean_forecast = forecast[\"0.5\"] if \"0.5\" in forecast.columns else forecast.iloc[:, 1]  # fallback\n",
        "\n",
        "        predictions_df = pd.DataFrame({\n",
        "            \"timestamp\": forecast[\"timestamp\"],\n",
        "            \"mean\": mean_forecast,\n",
        "            \"lower_bound\": lower_bound,\n",
        "            \"upper_bound\": upper_bound,\n",
        "        })\n",
        "\n",
        "        return predictions_df\n",
        "\n",
        "    def evaluate(self, test_data: pd.DataFrame, target_column: str = \"query_count\") -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluates the model using custom metrics: q-error, MAE, and RME.\n",
        "        Instead of merging on timestamps (which can fail if the test timestamps\n",
        "        do not match the forecast horizon timestamps), this version assumes that\n",
        "        the actual values for evaluation are the last `prediction_length` rows of\n",
        "        the prepared test data (sorted by timestamp).\n",
        "\n",
        "        Returns a dictionary with the computed metrics.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model must be trained before evaluation.\")\n",
        "\n",
        "        # Prepare the data in long format\n",
        "        prepared_data = self.prepare_data(test_data, target_column)\n",
        "        # Sort the prepared data to ensure chronological order\n",
        "        prepared_data = prepared_data.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "        # Get forecast predictions (using the mean forecast as our estimate)\n",
        "        predictions_df = self.predict(test_data, target_column)\n",
        "        predictions_df = predictions_df.reset_index(drop=True)\n",
        "\n",
        "        # Instead of merging by timestamp, assume that the actual values for the forecast horizon\n",
        "        # are the last `prediction_length` rows of the prepared data.\n",
        "        if len(prepared_data) < self.prediction_length:\n",
        "            raise ValueError(\"Not enough test data to cover the forecast horizon.\")\n",
        "\n",
        "        actual_forecast = prepared_data.iloc[-self.prediction_length:].reset_index(drop=True)\n",
        "\n",
        "        if len(actual_forecast) != len(predictions_df):\n",
        "            raise ValueError(\"Mismatch between forecast length and actuals length.\")\n",
        "\n",
        "        forecast = predictions_df[\"mean\"].values\n",
        "        actual = actual_forecast[target_column].values\n",
        "\n",
        "        # To avoid division by zero, add a small epsilon\n",
        "        epsilon = 1e-10\n",
        "\n",
        "        # Compute MAE\n",
        "        mae = np.mean(np.abs(forecast - actual))\n",
        "\n",
        "        # Compute q-error for each forecast point\n",
        "        q_errors = np.maximum(forecast / (actual + epsilon), actual / (forecast + epsilon))\n",
        "        q_error = np.mean(q_errors)\n",
        "\n",
        "        # Compute Relative Mean Error (RME)\n",
        "        rme = np.mean(np.abs(forecast - actual) / (np.abs(actual) + epsilon))\n",
        "\n",
        "        metrics = {\n",
        "            \"q_error\": q_error,\n",
        "            \"mae\": mae,\n",
        "            \"rme\": rme\n",
        "        }\n",
        "\n",
        "        print(\"Evaluation Metrics:\")\n",
        "        print(f\"Q-error: {q_error:.4f}\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"RME: {rme:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Saves the trained model to disk.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No trained model to save.\")\n",
        "\n",
        "        self.model.save()\n",
        "        print(f\"Model saved successfully.\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"\n",
        "        Loads a trained model from disk.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"No model file found at {path}\")\n",
        "\n",
        "        self.model = TimeSeriesPredictor.load(path)\n",
        "        print(f\"Model loaded from {path}.\")\n",
        "\n",
        "    def train_with_cv_and_tuning(\n",
        "        self,\n",
        "        train_data: pd.DataFrame,\n",
        "        target_column: str = \"query_count\",\n",
        "        hyperparams_list: List[Dict] = None,\n",
        "        num_val_windows: int = 3,\n",
        "        eval_metric: str = \"WQL\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Trains the DeepAR model with multiple hyperparameter configurations\n",
        "        and rolling-window cross-validation, then picks the best model.\n",
        "\n",
        "        :param train_data: The training DataFrame (with 'timestamp' and target_column).\n",
        "        :param target_column: The name of the target column.\n",
        "        :param hyperparams_list: A list of dictionaries of hyperparameters for DeepAR.\n",
        "        :param num_val_windows: Number of rolling windows to use for cross-validation.\n",
        "        :param eval_metric: The evaluation metric to use, e.g. \"WQL\", \"MASE\", etc.\n",
        "        \"\"\"\n",
        "        if hyperparams_list is None or len(hyperparams_list) == 0:\n",
        "            # default to a single set\n",
        "            hyperparams_list = [\n",
        "                {\n",
        "                    \"epochs\": 20,\n",
        "                    \"learning_rate\": 1e-3,\n",
        "                    \"num_layers\": 2,\n",
        "                    \"hidden_size\": 40,\n",
        "                    \"dropout_rate\": 0.1,\n",
        "                    \"batch_size\": 32\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        # Prepare data (long format)\n",
        "        prepared_data = self.prepare_data(train_data, target_column=target_column)\n",
        "\n",
        "        # Construct a hyperparameters dict recognized by AutoGluon (list under \"DeepAR\" key)\n",
        "        hyperparams = {\"DeepAR\": hyperparams_list}\n",
        "\n",
        "        # Create a new predictor\n",
        "        from autogluon.timeseries import TimeSeriesPredictor\n",
        "\n",
        "        predictor = TimeSeriesPredictor(\n",
        "            target=target_column,\n",
        "            prediction_length=self.prediction_length,\n",
        "            freq=self.freq,\n",
        "            eval_metric=eval_metric\n",
        "        )\n",
        "\n",
        "        # Fit with cross-validation (num_val_windows) and the given hyperparams\n",
        "        predictor.fit(\n",
        "            train_data=prepared_data,\n",
        "            hyperparameters=hyperparams,\n",
        "            num_val_windows=num_val_windows,\n",
        "            verbosity=2  # for more detailed logs\n",
        "        )\n",
        "\n",
        "        # Store the best predictor in self.model\n",
        "        self.model = predictor\n",
        "        print(\"Cross-validation and hyperparameter tuning complete. Best model stored in self.model.\")"
      ],
      "metadata": {
        "id": "iCSJx-f6-Vm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class DataLoadError(Exception):\n",
        "    \"\"\"Custom error raised when there is a problem loading the data.\"\"\"\n",
        "\n",
        "    def __init__(self, message, code=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            message (str): A descriptive error message.\n",
        "            code (int, optional): An optional error code for more granular error handling.\n",
        "        \"\"\"\n",
        "        # Capture the current timestamp\n",
        "        self.timestamp = datetime.now()\n",
        "        self.message = message\n",
        "        self.code = code\n",
        "\n",
        "        # Format the error message with timestamp and error code (if provided)\n",
        "        timestamp_str = self.timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        if code is not None:\n",
        "            full_message = f\"[{timestamp_str}] [Error {code}] {message}\"\n",
        "        else:\n",
        "            full_message = f\"[{timestamp_str}] {message}\"\n",
        "\n",
        "        super().__init__(full_message)\n",
        "\n",
        "    def log_error(self):\n",
        "        \"\"\"Log the error details along with the timestamp.\"\"\"\n",
        "        print(f\"Error Logged at {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}: {self}\")\n",
        "\n",
        "class DataSplitError(Exception):\n",
        "    \"\"\"Custom error raised when there is an issue splitting the dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, message, code=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            message (str): A descriptive error message.\n",
        "            code (int, optional): An optional error code for more granular error handling.\n",
        "        \"\"\"\n",
        "        self.timestamp = datetime.now()\n",
        "        self.message = message\n",
        "        self.code = code\n",
        "\n",
        "        timestamp_str = self.timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        if code is not None:\n",
        "            full_message = f\"[{timestamp_str}] [Error {code}] {message}\"\n",
        "        else:\n",
        "            full_message = f\"[{timestamp_str}] {message}\"\n",
        "\n",
        "        super().__init__(full_message)\n",
        "\n",
        "    def log_error(self):\n",
        "        \"\"\"Log the error details along with the timestamp.\"\"\"\n",
        "        print(f\"Error Logged at {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}: {self}\")"
      ],
      "metadata": {
        "id": "yHbsSR3e_moS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# from utility.errors import (\n",
        "#     DataLoadError,\n",
        "#     DataSplitError,\n",
        "# )\n",
        "\n",
        "\n",
        "class DataManager:\n",
        "    \"\"\"Helper class for loading data and creating train/test splits.\"\"\"\n",
        "\n",
        "    def __init__(self, cluster_type, instance_id):\n",
        "        \"\"\"\n",
        "        Initialize the DataManager with a specific cluster type and instance ID.\n",
        "\n",
        "        Args:\n",
        "            cluster_type (str): either 'serverless' or 'provisioned'.\n",
        "            instance_id (int): indicates which instance is being considered.\n",
        "        \"\"\"\n",
        "        self.cluster_type = cluster_type\n",
        "        self.instance_id = instance_id\n",
        "        self.data = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads the data into a dataframe for training and testing.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A dataframe with columns:\n",
        "                - instance_id: id of the instance\n",
        "                - timestamp: hourly timestamp in the format 'YYYY-MM-DD HH'\n",
        "                - query_count: total number of queries in that hour\n",
        "                - runtime: combined execution time of all queries in that hour\n",
        "                - bytes_scanned: total amount of Gigabytes scanned in that hour\n",
        "\n",
        "        Raises:\n",
        "            DataLoadError: If the cluster_type is invalid, no appropriate file is found,\n",
        "                           or if the file's columns do not match the expected ones.\n",
        "        \"\"\"\n",
        "        # Validate cluster_type\n",
        "        if self.cluster_type not in [\"serverless\", \"provisioned\"]:\n",
        "            raise DataLoadError(\n",
        "                \"Invalid cluster_type. Must be 'serverless' or 'provisioned'.\",\n",
        "                code=1001,\n",
        "            )\n",
        "\n",
        "        # Define expected filenames based on cluster_type\n",
        "        parquet_file = f\"{self.cluster_type}.parquet\"\n",
        "        csv_file = f\"{self.cluster_type}.csv\"\n",
        "\n",
        "        # Attempt to load the dataframe from the available file\n",
        "        if os.path.exists(parquet_file):\n",
        "            try:\n",
        "                df = pd.read_parquet(parquet_file)\n",
        "            except Exception as e:\n",
        "                raise DataLoadError(\n",
        "                    f\"Error reading the parquet file: {e}\", code=1002\n",
        "                )\n",
        "        elif os.path.exists(csv_file):\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "            except Exception as e:\n",
        "                raise DataLoadError(\n",
        "                    f\"Error reading the CSV file: {e}\", code=1003\n",
        "                )\n",
        "        else:\n",
        "            raise DataLoadError(\n",
        "                f\"Neither {parquet_file} nor {csv_file} was found in the working directory.\",\n",
        "                code=1004,\n",
        "            )\n",
        "\n",
        "        # Define the expected columns\n",
        "        expected_columns = {\n",
        "            \"instance_id\",\n",
        "            \"timestamp\",\n",
        "            \"query_count\",\n",
        "            \"runtime\",\n",
        "            \"bytes_scanned\",\n",
        "        }\n",
        "\n",
        "        # Check if dataframe columns exactly match the expected columns\n",
        "        if set(df.columns) != expected_columns or len(df.columns) != len(\n",
        "            expected_columns\n",
        "        ):\n",
        "            raise DataLoadError(\n",
        "                \"The loaded dataframe does not have the required columns.\",\n",
        "                code=1005,\n",
        "            )\n",
        "\n",
        "        # Filter the dataframe to only include rows with the specified instance_id\n",
        "        self.data = df[df[\"instance_id\"] == self.instance_id]\n",
        "        return self.data\n",
        "\n",
        "    def train_test_split(self, data=None):\n",
        "        \"\"\"\n",
        "        Splits the data into two training and test sets. The first training set spans the first N-2 weeks,\n",
        "        and the second training set spans the first N-1 weeks. The test sets consist of the week immediately\n",
        "        following the respective training set.\n",
        "\n",
        "        Args:\n",
        "            data (pd.DataFrame, optional): DataFrame to be split. If not provided, uses the loaded data.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (first training set, first test set, second training set, second test set)\n",
        "\n",
        "        Raises:\n",
        "            DataSplitError: If there are fewer than 3 weeks in the data.\n",
        "        \"\"\"\n",
        "        # Use the loaded data if none is provided\n",
        "        if data is None:\n",
        "            if self.data is None:\n",
        "                raise DataSplitError(\n",
        "                    \"No data available to split. Please load data first.\",\n",
        "                    code=2001,\n",
        "                )\n",
        "            data = self.data.copy()\n",
        "        else:\n",
        "            data = data.copy()\n",
        "\n",
        "        # Ensure that the 'timestamp' column is in datetime format\n",
        "        if not pd.api.types.is_datetime64_any_dtype(data[\"timestamp\"]):\n",
        "            data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n",
        "\n",
        "        # reset the latest timestamp to midnight. This ensures that\n",
        "        # only full days are included<s\n",
        "        latest_date = data[\"timestamp\"].max().normalize()\n",
        "        data = data[data[\"timestamp\"] < latest_date]\n",
        "\n",
        "        # Create a new column 'week' representing the week period (using ISO week)\n",
        "        data[\"week\"] = data[\"timestamp\"].dt.to_period(\"W\")\n",
        "\n",
        "        # Get a sorted list of unique weeks in the data\n",
        "        unique_weeks = sorted(data[\"week\"].unique())\n",
        "        N = len(unique_weeks)\n",
        "\n",
        "        if N < 3:\n",
        "            raise DataSplitError(\n",
        "                \"Not enough weeks in data for splitting. Need at least 3 weeks.\",\n",
        "                code=2000,\n",
        "            )\n",
        "\n",
        "        # First split:\n",
        "        #   Training set: weeks[0] to weeks[N-3] (i.e. first N-2 weeks)\n",
        "        #   Test set: the week immediately after, i.e. week at index N-2\n",
        "        train1_weeks = unique_weeks[: N - 2]\n",
        "        test1_week = unique_weeks[N - 2]\n",
        "\n",
        "        # Second split:\n",
        "        #   Training set: weeks[0] to weeks[N-2] (i.e. first N-1 weeks)\n",
        "        #   Test set: the week immediately after, i.e. the last week\n",
        "        train2_weeks = unique_weeks[: N - 1]\n",
        "        test2_week = unique_weeks[-1]\n",
        "\n",
        "        # Create the splits based on the week periods\n",
        "        train1 = data[data[\"week\"].isin(train1_weeks)].copy()\n",
        "        test1 = data[data[\"week\"] == test1_week].copy()\n",
        "        train2 = data[data[\"week\"].isin(train2_weeks)].copy()\n",
        "        test2 = data[data[\"week\"] == test2_week].copy()\n",
        "\n",
        "        for df in (train1, test1, train2, test2):\n",
        "            df.drop(columns=[\"week\"], inplace=True)\n",
        "\n",
        "        return train1, test1, train2, test2\n"
      ],
      "metadata": {
        "id": "4kgGw-Ql_YZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U5OUvSi9HlK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "# from utility.helpers import DataManager\n",
        "# import visualization\n",
        "# from utility.baseline_models import DeepAR\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "datamanager = DataManager('provisioned', 96)\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM8TTHmY9HlL"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = datamanager.load_data()\n",
        "# Convert timestamp to datetime\n",
        "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
        "# Sort the data by timestamp\n",
        "data = data.sort_values('timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWf7b4-L9HlM"
      },
      "outputs": [],
      "source": [
        "# Visualize data\n",
        "# visualization.visualize_data(data)\n",
        "data\n",
        "import numpy as np\n",
        "data['query_count'] = np.log1p(data['query_count'])  # Apply log transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANwV8h5f9HlM"
      },
      "outputs": [],
      "source": [
        "# Split into training and test data:\n",
        "# Following the approach in the paper (p. 132), for a cluster with N weeks of data,\n",
        "# the first train-test split includes N-2 weeks for training and the following week\n",
        "# for testing. The second train-test split contains the first N-1 weeks for training\n",
        "# and the following week for testing, representing a scenario of re-training a model\n",
        "# each week and forecasting for the next week\n",
        "\n",
        "train1, test1, train2, test2 = datamanager.train_test_split(data)\n",
        "\n",
        "print(f\"train1 shape: {train1.shape}\")\n",
        "print(f\"test1 shape: {test1.shape}\")\n",
        "print(f\"train2 shape: {train2.shape}\")\n",
        "print(f\"test2 shape: {test2.shape}\")\n",
        "\n",
        "print(train1.iloc[-1])\n",
        "print(test1.iloc[0])\n",
        "print(train1.iloc[-1])\n",
        "print(test1.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfDYDYVi9HlM"
      },
      "source": [
        "## Baseline Model: DeepAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhjeJi9r9HlM"
      },
      "outputs": [],
      "source": [
        "print(train1['timestamp'].max(), test1['timestamp'].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wzPPs1f9HlN"
      },
      "outputs": [],
      "source": [
        "print(train1.isnull().sum())  # Check for NaNs\n",
        "train_data = train1.fillna(0)  # Fill missing values with zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itqvqMCe9HlN"
      },
      "outputs": [],
      "source": [
        "# Define the forecast horizon (e.g., forecast the next 48 hours)\n",
        "prediction_length = 168\n",
        "\n",
        "# Forecast should start one hour after the last training timestamp\n",
        "start_forecast = test1['timestamp'].min()\n",
        "end_forecast = start_forecast + pd.Timedelta(hours=prediction_length - 1)\n",
        "print(\"Forecast horizon:\", start_forecast, \"to\", end_forecast)\n",
        "\n",
        "# Filter test1 to only include rows within the forecast horizon\n",
        "test_forecast = test1[(test1['timestamp'] >= start_forecast) & (test1['timestamp'] <= end_forecast)]\n",
        "print(\"Filtered test_forecast timestamps:\")\n",
        "print(test_forecast[['timestamp', 'query_count']].count())\n",
        "\n",
        "hyperparameters = {\n",
        "    'DeepAR': {\n",
        "        'num_layers': 2,          # Increase to 3 layers for better representation\n",
        "        'hidden_size': 40,        # Slightly larger hidden size\n",
        "        'dropout_rate': 0.2,      # Higher dropout to prevent overfitting\n",
        "        'learning_rate': 5e-4,    # Reduce learning rate for stable training\n",
        "        'patience': 5,           # Increase patience for early stopping\n",
        "        'max_epochs': 50,        # More epochs for better convergence\n",
        "        'context_length': 168,\n",
        "        'use_feat_dynamic_real': True,  # Use additional features (hour, day_of_week),\n",
        "        'batch_size': 16,         # Reduce batch size for training stability\n",
        "        'freq': 'H',  # Use uppercase \"H\" for hourly data\n",
        "        'verbosity': 2\n",
        "    }\n",
        "}\n",
        "\n",
        "# os.environ[\"AUTOGLUON_DEVICE\"] = \"cpu\"\n",
        "\n",
        "# Enable logging (Fix: Force verbose output)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logging.getLogger(\"autogluon\").setLevel(logging.DEBUG)\n",
        "\n",
        "# Instantiate the DeepAR model using AutoGluon\n",
        "model = DeepAR(prediction_length=prediction_length, freq=\"h\",hyperparameters=hyperparameters)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.train(train1, target_column=\"query_count\")\n",
        "\n",
        "# Generate predictions on the filtered test data (forecast horizon)\n",
        "predictions_df = model.predict(test_forecast, target_column=\"query_count\")\n",
        "print(\"Predictions:\")\n",
        "print(predictions_df.head())\n",
        "\n",
        "# # Evaluate the model on the filtered test data\n",
        "# evaluation_results = model.evaluate(test_forecast, target_column=\"query_count\")\n",
        "# print(\"Evaluation Metrics:\")\n",
        "# print(evaluation_results)\n",
        "\n",
        "# Save the trained model to disk\n",
        "model.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egWuj1qg9HlN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Optionally, convert timestamps to datetime if they aren't already\n",
        "test1['timestamp'] = pd.to_datetime(test1['timestamp'])\n",
        "predictions_df['timestamp'] = pd.to_datetime(predictions_df['timestamp'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(test_forecast['timestamp'], test_forecast['query_count'], label='Actual')\n",
        "plt.plot(predictions_df['timestamp'], predictions_df['mean'], label='Predicted', linestyle='--', color='red')\n",
        "\n",
        "# Fill the confidence interval if available\n",
        "plt.fill_between(predictions_df['timestamp'],\n",
        "                 predictions_df['lower_bound'],\n",
        "                 predictions_df['upper_bound'],\n",
        "                 color='red', alpha=0.3, label='Confidence Interval')\n",
        "\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Query Count')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = model.evaluate(test_forecast, target_column=\"query_count\")\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "yEuWXirICgrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Forecast Shape:\", test_forecast.shape)\n",
        "print(\"Predictions Shape:\", predictions_df.shape)\n",
        "\n",
        "print(\"Test Forecast Head:\")\n",
        "print(test_forecast.head())\n",
        "\n",
        "print(\"Predictions DataFrame Head:\")\n",
        "print(predictions_df.head())\n",
        "\n",
        "print(\"Prediction timestamps range:\", predictions_df[\"timestamp\"].min(), \"to\", predictions_df[\"timestamp\"].max())\n",
        "print(\"Test data timestamps range:\", test_forecast[\"timestamp\"].min(), \"to\", test_forecast[\"timestamp\"].max())"
      ],
      "metadata": {
        "id": "vJvNjfsXCsaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plPxPvjhPzNF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}